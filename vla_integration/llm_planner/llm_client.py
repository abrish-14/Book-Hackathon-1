"""
LLM client for VLA Integration Module.
Handles interaction with OpenAI models for cognitive planning.
"""
import openai
import os
import logging
from typing import Optional, List, Dict, Any
from dataclasses import dataclass
import json

from vla_integration.config.settings import config


@dataclass
class TaskPlan:
    """
    Data class to hold a task plan generated by the LLM.
    """
    id: str
    original_command: str
    tasks: List[Dict[str, Any]]
    validation_status: str  # valid, invalid, pending
    robot_compatibility: bool
    confidence: float  # 0.0 to 1.0


class LLMClient:
    """
    Client for interacting with OpenAI models for cognitive planning and task generation.
    """

    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None, temperature: Optional[float] = None):
        """
        Initialize the LLM client.

        Args:
            api_key: OpenAI API key. If not provided, will use config value or environment variable.
            model: OpenAI model to use. Uses config value if None.
            temperature: Temperature for the model. Uses config value if None.
        """
        # Use provided API key, config value, or environment variable
        if api_key:
            self.api_key = api_key
        elif config.openai_api_key:
            self.api_key = config.openai_api_key
        elif os.getenv("OPENAI_API_KEY"):
            self.api_key = os.getenv("OPENAI_API_KEY")
        else:
            raise ValueError("OpenAI API key must be provided or set in configuration or OPENAI_API_KEY environment variable")

        # Initialize the OpenAI client
        self.client = openai.OpenAI(api_key=self.api_key)

        # Set model and other parameters
        self.model = model or config.llm_model
        self.temperature = temperature or config.llm_temperature
        self.max_tokens = config.llm_max_tokens

        # Setup logging
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(config.log_level)
        handler = logging.StreamHandler()
        formatter = logging.Formatter(config.log_format)
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)

    def generate_task_plan(self, command: str, context: Optional[Dict[str, Any]] = None) -> TaskPlan:
        """
        Generate a task plan from a natural language command using an LLM.

        Args:
            command: Natural language command to process
            context: Additional context for planning (optional)

        Returns:
            TaskPlan containing the sequence of tasks to execute
        """
        # Create the system message to guide the LLM
        system_message = """
        You are a helpful assistant that converts natural language commands into structured action plans for humanoid robots.
        Your response should be a JSON object with a 'tasks' key containing an array of action objects.
        Each action object should have:
        - 'action_type': A string representing the type of action (e.g., 'move', 'grasp', 'navigate', 'speak', 'wait')
        - 'parameters': An object containing the parameters for the action
        - 'priority': An integer indicating execution priority (lower numbers first)
        - 'dependencies': An array of task IDs that this task depends on (empty if no dependencies)

        Example response format:
        {
          "tasks": [
            {
              "action_type": "navigate",
              "parameters": {
                "target_location": "kitchen",
                "speed": 1.0
              },
              "priority": 1,
              "dependencies": []
            },
            {
              "action_type": "grasp",
              "parameters": {
                "object": "red cup",
                "arm": "right"
              },
              "priority": 2,
              "dependencies": ["task_1"]
            }
          ]
        }

        Only respond with valid JSON, no other text.
        """

        # Create the user message with the command
        user_message = f"Command: {command}\n"
        if context:
            user_message += f"Context: {json.dumps(context)}\n"
        user_message += "Generate a structured action plan for a humanoid robot."

        try:
            # Call the OpenAI API using the new client format
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": user_message}
                ],
                temperature=self.temperature,  # Use configured temperature
                max_tokens=self.max_tokens,
                response_format={"type": "json_object"}
            )

            # Extract the response content
            response_content = response.choices[0].message.content.strip()

            # Parse the JSON response
            parsed_response = json.loads(response_content)

            # Extract tasks from the response
            tasks = parsed_response.get('tasks', [])

            # Create and return the TaskPlan
            return TaskPlan(
                id=f"plan_{len(command)}_{hash(command) % 10000}",
                original_command=command,
                tasks=tasks,
                validation_status="pending",
                robot_compatibility=False,  # Will be validated separately
                confidence=0.8  # Default confidence, could be calculated based on response metrics
            )

        except json.JSONDecodeError as e:
            self.logger.error(f"Error parsing LLM response as JSON: {e}")
            self.logger.error(f"Raw response: {response_content}")
            # Return a default task plan with an error
            return TaskPlan(
                id=f"error_plan_{hash(command) % 10000}",
                original_command=command,
                tasks=[],
                validation_status="invalid",
                robot_compatibility=False,
                confidence=0.0
            )
        except Exception as e:
            self.logger.error(f"Error generating task plan: {e}")
            # Return a default task plan with an error
            return TaskPlan(
                id=f"error_plan_{hash(command) % 10000}",
                original_command=command,
                tasks=[],
                validation_status="invalid",
                robot_compatibility=False,
                confidence=0.0
            )

    def validate_task_plan(self, task_plan: TaskPlan, robot_capabilities: Optional[Dict[str, Any]] = None) -> TaskPlan:
        """
        Validate a task plan against robot capabilities.

        Args:
            task_plan: The task plan to validate
            robot_capabilities: Dictionary of robot capabilities to validate against

        Returns:
            Updated TaskPlan with validation status
        """
        if not robot_capabilities:
            # Default capabilities if none provided
            robot_capabilities = {
                "actions": ["move", "grasp", "navigate", "speak", "wait"],
                "max_speed": 2.0,
                "arm_reach": 1.5,
                "max_payload": 5.0
            }

        # Validate each task against robot capabilities
        valid_tasks = []
        for task in task_plan.tasks:
            action_type = task.get('action_type', '')
            parameters = task.get('parameters', {})

            # Check if action type is supported
            if action_type not in robot_capabilities.get('actions', []):
                print(f"Warning: Action type '{action_type}' not supported by robot")
                continue

            # Validate specific parameters based on action type
            if action_type == 'move' and 'speed' in parameters:
                if parameters['speed'] > robot_capabilities.get('max_speed', 2.0):
                    print(f"Warning: Speed {parameters['speed']} exceeds robot's max speed")
                    parameters['speed'] = robot_capabilities['max_speed']

            if action_type == 'grasp' and 'object' in parameters:
                # Could validate payload or object size here
                pass

            # Add the validated task
            valid_tasks.append(task)

        # Update the task plan with validated tasks
        task_plan.tasks = valid_tasks
        task_plan.robot_compatibility = len(valid_tasks) > 0
        task_plan.validation_status = "valid" if task_plan.robot_compatibility else "invalid"

        return task_plan

    def refine_plan_with_context(self, task_plan: TaskPlan, context: Dict[str, Any]) -> TaskPlan:
        """
        Refine a task plan based on additional context information.

        Args:
            task_plan: The original task plan to refine
            context: Context information to consider

        Returns:
            Refined TaskPlan
        """
        # In a real implementation, this would use the LLM to refine the plan based on context
        # For now, we'll just return the original plan
        return task_plan

    def get_action_description(self, task: Dict[str, Any]) -> str:
        """
        Get a human-readable description of an action.

        Args:
            task: The task dictionary

        Returns:
            Human-readable description of the action
        """
        action_type = task.get('action_type', 'unknown')
        parameters = task.get('parameters', {})

        if action_type == 'move':
            direction = parameters.get('direction', 'forward')
            distance = parameters.get('distance', 1.0)
            return f"Move {direction} {distance} meters"
        elif action_type == 'navigate':
            location = parameters.get('target_location', 'unknown')
            return f"Navigate to {location}"
        elif action_type == 'grasp':
            obj = parameters.get('object', 'unknown object')
            return f"Grasp {obj}"
        elif action_type == 'speak':
            text = parameters.get('text', 'unknown text')
            return f"Speak: {text}"
        elif action_type == 'wait':
            duration = parameters.get('duration', 1.0)
            return f"Wait for {duration} seconds"
        else:
            return f"Perform {action_type} action"


# Example usage:
if __name__ == "__main__":
    # Initialize LLM client
    # llm_client = LLMClient(api_key="your-api-key-here", model="gpt-3.5-turbo")
    print("LLM client initialized.")